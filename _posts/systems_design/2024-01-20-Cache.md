---
title:  "Caches"
date:   2024-01-20 21:45:00
categories: [System Design]
tags: [System Design]
---
{% include toc title="Index" %}

Temporary storage that stores a part of the DB
- reduces the response time
- reduces the load on the database

### Global Cache vs Local Cache

**Local Cache** 
- in-memory
- no need for any network calls for the cache storage if its in RAM

**Global Cache** 
- Redis or Memcached
- Redis as a Global Data cache.
- Can be scaled independently

Cache Eviction and Loading Policy


# Write Policies in a cache
A write request means some entry is `added, updated or deleted` in the cache


Write policy is when there is a write operation in a cache, usually a user operation

Create
Delete
Update


Write back
timeout based persistence
Cache is always updated
DB is eventually consistent

### Write-Back Policy

- If the key-value pair to be updated, is present in the cache then it is updated. 
  - the key-value pair is not immediately updated in the database. 

**Timeout-Based Persistence**. In this mechanism, we have a TTL(Time to Live for each entry in cache). 
If the timestamp becomes greater than the TTL, the entry is evicted and the data is persisted in the database. 

This makes the database eventually consistent

**Event-Based Write Back**. In this mechanism, keep an event-based trigger along with Key-Value Pair. 
For example, # of updates as a trigger, if it becomes greater than 5 -> triggers to persist cache value in the database

**Replacement Write Back**. For each entry in the cache, we have an attribute that tells us if the entry is updated or not. 
When we want to evict the entry we update the entry in the database if the entry was updated in the cache


### Write-Through Policy

when there is a write request, we evict the key that is being updated, while simultaneously updating the database. 

The next time there is a read request, the cache polls the database and receives the response.

- lock the data which is to be written and only unlock the data after the update operation is completed.
- provides a high level of consistency
- and a high level of persistence
- Less efficient compared to the write-back policy

### Write-Around Policy

instead of updating the entry in the cache, we update the entry in the database.

Now when we get a read request,
we will send the stale value from the cache.

And we will be getting stale value until the entry is evicted from the cach

# Cache Replacement Policies
Policies to Evict the data from the cache.

A replacement policy is triggered when there is no space for a newkey and a key is evicted from the cache
Replacement policy is when a new entry is copied from the DB



### LRU - Least Recently Used
evict the entry that has not been used for the **longest**

### LFU Policy (Least Frequently Used)

evict the entry that is used the least number of times (frequency)

Add a frequency data point and determine which pair is to be evicted

In most cases,**_LRU Policy works better than LFU_**

# Thrashing 
- When the miss ratio is high and there are a lot of loads and evictions
- Cache never has required data and DB is asked everytime

# Replacement Policy in Memcached

Mem-cached has three regions-cold, warm and hot.The hot region does not contain the Last Used timestamp.

As new entries come in,entries in the hot region are pushed to the warm or cold region

One data store is used to store entries that are less requested (cold region)
Another data store is used to store entries that are more requested (warm region)

Entries gets promoted based on the usage from Cold to Warm regions

whether the entry lives in the hot or cold region is decided by the frequency of usage.
And whether the entry needs to be evicted is decided by its LastUsed timestamp

This process is known as Segmented LRU.

[https://redis.com/blog/count-min-sketch-the-art-and-science-of-estimating-stuff/](https://redis.com/blog/count-min-sketch-the-art-and-science-of-estimating-stuff/)

